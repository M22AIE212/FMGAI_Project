{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d31053-5b72-431d-baaa-f8b30fa182c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ced107-0b1d-4c14-8acb-c895a24b7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install jupyterlab=2.1.5 ipympl==0.5.6   \n",
    "Loading widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5036aae-e20c-438a-bab0-2ecb3736be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor\n",
    "from dataclasses import dataclass\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from pyexpat import features\n",
    "import copy\n",
    "import math\n",
    "from sys import prefix\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BlipModel, AutoConfig, AutoModel\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import FlavaModel, FlavaProcessor\n",
    "import copy\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0a2f52-67f5-41d2-8dd2-3406bc615d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://sysproxy.wal-mart.com:8080\"\n",
    "os.environ['https_proxy'] = \"http://sysproxy.wal-mart.com:8080\"\n",
    "os.environ['NO_PROXY'] = \"*.walmart.com,*.wal-mart.com,*.walmart.net,*.wal-mart.net,localhost,127.0.0.1\"\n",
    "os.environ['NO_PROXY'] = \\\n",
    "\"*.walmart.com*.wal-mart.com*.walmart.net*.wal-mart.netlocalhost127.0.0.1.cdn-lfs.hf.co.api.agentops.ai\"\n",
    "os.environ[\"SSL_CERT_FILE\"] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "# os.environ['SSL_CERT_FILE'] = '/etc/ssl/certs/ca-bundle.crt'\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "# os.environ['no_proxy'] = \"cdn-lfs.hf.co\"\n",
    "# os.environ['no_proxy'] = \"*.wal-mart.com,*.walmart.com,*.walmart.net,*.wal-mart.net\"\n",
    "# os.environ['no_proxy'] = \"cdn-lfs.hf.co\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd8a8a77-feca-4d76-b946-f4d34ca68d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abo-images-small.tar', <http.client.HTTPMessage at 0x7fc9b8cfe7d0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# url = \"https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\"\n",
    "# filename = \"abo-images-small.tar\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b25d43-0d1f-4010-a84b-76259597348e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abo-listings.tar', <http.client.HTTPMessage at 0x7fc9ab5c4d90>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# url = \"https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-listings.tar\"\n",
    "# filename = \"abo-listings.tar\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34911ce-58f7-47e7-b32d-36354f4e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     \n",
    "shutil.unpack_archive(\"/data/Kishore/Major/abo-images-small.tar\", \"/data/Kishore/Major/abo-images-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6ee59-4621-41ff-97b9-7b341e2450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"/data/Kishore/Major/abo-listings.tar\", \"/data/Kishore/Major/abo-listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51dc383-5bce-435a-87eb-46345169e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: listings_f.json.gz\n",
      "Reading file: listings_e.json.gz\n",
      "Reading file: listings_d.json.gz\n",
      "Reading file: listings_c.json.gz\n",
      "Reading file: listings_b.json.gz\n",
      "Reading file: listings_a.json.gz\n",
      "Reading file: listings_9.json.gz\n",
      "Reading file: listings_8.json.gz\n",
      "Reading file: listings_7.json.gz\n",
      "Reading file: listings_6.json.gz\n",
      "Reading file: listings_5.json.gz\n",
      "Reading file: listings_4.json.gz\n",
      "Reading file: listings_3.json.gz\n",
      "Reading file: listings_2.json.gz\n",
      "Reading file: listings_1.json.gz\n",
      "Reading file: listings_0.json.gz\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing .json.gz files\n",
    "folder_path = '/data/Kishore/Major/abo-listings/listings/metadata'\n",
    "data_dict = dict()\n",
    "# Iterate through all the files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json.gz'):  # Check if the file is a .json.gz file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Open and read the gzipped JSON file line by line\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as gz_file:\n",
    "            print(f\"Reading file: {file_name}\")\n",
    "            for idx , line in enumerate(gz_file):  # Read each line\n",
    "                try:\n",
    "                    data = json.loads(line)  # Load each line as a separate JSON object\n",
    "                    for item_name in data['item_name'] :\n",
    "                      if('en_' in item_name['language_tag']) :\n",
    "                        data_dict[f\"{file_name}_{idx}\"] = dict()\n",
    "                        data_dict[f\"{file_name}_{idx}\"][\"product_name\"] = item_name['value']\n",
    "                        data_dict[f\"{file_name}_{idx}\"][\"hierarchy\"] = data.get('node',None)\n",
    "                        all_image_id = [data.get('main_image_id',None)]\n",
    "                        if data.get('other_image_id',None) :\n",
    "                          pass\n",
    "                          # all_image_id.extend(data.get('other_image_id',None))\n",
    "                        data_dict[f\"{file_name}_{idx}\"][\"image_id\"] = all_image_id\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Could not parse JSON in file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43d699e-8963-48ab-b6c8-beb3be031b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for key in data_dict.keys() :\n",
    "    d = data_dict[key]\n",
    "    d['key'] = key\n",
    "    all_data.append(d)\n",
    "\n",
    "ecomm_data = pd.DataFrame((all_data))\n",
    "ecomm_data = ecomm_data.explode('image_id')\n",
    "ecomm_data = ecomm_data.dropna()\n",
    "ecomm_data.drop(columns = ['key'],inplace = True)\n",
    "ecomm_data[\"clean_hierarchy\"] = ecomm_data.hierarchy.apply(lambda x : x[-1]['node_name'])\n",
    "ecomm_data = ecomm_data.dropna()\n",
    "ecomm_data = ecomm_data[ecomm_data.clean_hierarchy.str.find('カテゴリー別') == -1]\n",
    "ecomm_data[\"target_class\"] = ecomm_data.clean_hierarchy.apply(lambda text :  text.split(\"/\")[-2])\n",
    "ecomm_data = ecomm_data[ecomm_data['target_class'] != \"\"]\n",
    "\n",
    "ecomm_data_agg = ecomm_data.groupby('target_class').agg({'target_class':'count'}).rename(columns = {'target_class':'count'}).reset_index()\n",
    "target_class_list = ecomm_data_agg[ecomm_data_agg['count'] > 222].target_class.unique()\n",
    "\n",
    "ecomm_data = ecomm_data[ecomm_data.target_class.isin(target_class_list)]\n",
    "\n",
    "ecomm_data_rest = ecomm_data[ecomm_data.target_class != 'Cases & Covers']\n",
    "ecomm_data_cc = ecomm_data[ecomm_data.target_class == 'Cases & Covers']\n",
    "\n",
    "ecomm_data = pd.concat([ecomm_data_rest,ecomm_data_cc.sample(3000)]).reset_index(drop =True)\n",
    "\n",
    "\n",
    "image_metadata = pd.read_csv(\"/data/Kishore/Major/abo-images-small/images/metadata/images.csv.gz\")\n",
    "ecomm_data_final = pd.merge(ecomm_data,image_metadata[['image_id','path']],on = 'image_id',how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006fd62d-90d6-4eab-8482-63ff0fc8273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25037, 6) 34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>hierarchy</th>\n",
       "      <th>image_id</th>\n",
       "      <th>clean_hierarchy</th>\n",
       "      <th>target_class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thirty Five Kent Men's Cashmere Zig Zag Scarf,...</td>\n",
       "      <td>[{'node_id': 10287217011, 'node_name': '/Categ...</td>\n",
       "      <td>81agKrncxwL</td>\n",
       "      <td>/Categories/Men/Clothing</td>\n",
       "      <td>Men</td>\n",
       "      <td>39/39c3dd60.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teen Mix By Catwalk Women's Pink Fashion Slipp...</td>\n",
       "      <td>[{'node_id': 1983639031, 'node_name': '/Catego...</td>\n",
       "      <td>81oct5RNPzL</td>\n",
       "      <td>/Categories/Shoes/Women's Shoes/Fashion Slippers</td>\n",
       "      <td>Women's Shoes</td>\n",
       "      <td>04/04e7dc58.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Brand - Solimo Designer Leather Texture...</td>\n",
       "      <td>[{'node_id': 1389409031, 'node_name': '/Catego...</td>\n",
       "      <td>71Qv6mabxdL</td>\n",
       "      <td>/Categories/Mobiles &amp; Accessories/Mobile Acces...</td>\n",
       "      <td>Mobile Accessories</td>\n",
       "      <td>7b/7bcc397a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AmazonBasics LED GU5.3 MR16 Spotlight Bulb, 4....</td>\n",
       "      <td>[{'node_id': 2314207011, 'node_name': '/Catego...</td>\n",
       "      <td>71jIZHHhUTL</td>\n",
       "      <td>/Categories/Light Bulbs/LED Bulbs</td>\n",
       "      <td>Light Bulbs</td>\n",
       "      <td>57/57bd086c.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon Brand - Symbol Men's Navy Canvas Sneake...</td>\n",
       "      <td>[{'node_id': 1983577031, 'node_name': '/Catego...</td>\n",
       "      <td>81lzP1TfGmL</td>\n",
       "      <td>/Categories/Shoes/Men's Shoes/Casual Shoes/Sne...</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>29/29ed0870.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  \\\n",
       "0  Thirty Five Kent Men's Cashmere Zig Zag Scarf,...   \n",
       "1  Teen Mix By Catwalk Women's Pink Fashion Slipp...   \n",
       "2  Amazon Brand - Solimo Designer Leather Texture...   \n",
       "3  AmazonBasics LED GU5.3 MR16 Spotlight Bulb, 4....   \n",
       "4  Amazon Brand - Symbol Men's Navy Canvas Sneake...   \n",
       "\n",
       "                                           hierarchy     image_id  \\\n",
       "0  [{'node_id': 10287217011, 'node_name': '/Categ...  81agKrncxwL   \n",
       "1  [{'node_id': 1983639031, 'node_name': '/Catego...  81oct5RNPzL   \n",
       "2  [{'node_id': 1389409031, 'node_name': '/Catego...  71Qv6mabxdL   \n",
       "3  [{'node_id': 2314207011, 'node_name': '/Catego...  71jIZHHhUTL   \n",
       "4  [{'node_id': 1983577031, 'node_name': '/Catego...  81lzP1TfGmL   \n",
       "\n",
       "                                     clean_hierarchy        target_class  \\\n",
       "0                           /Categories/Men/Clothing                 Men   \n",
       "1   /Categories/Shoes/Women's Shoes/Fashion Slippers       Women's Shoes   \n",
       "2  /Categories/Mobiles & Accessories/Mobile Acces...  Mobile Accessories   \n",
       "3                  /Categories/Light Bulbs/LED Bulbs         Light Bulbs   \n",
       "4  /Categories/Shoes/Men's Shoes/Casual Shoes/Sne...        Casual Shoes   \n",
       "\n",
       "              path  \n",
       "0  39/39c3dd60.jpg  \n",
       "1  04/04e7dc58.jpg  \n",
       "2  7b/7bcc397a.jpg  \n",
       "3  57/57bd086c.jpg  \n",
       "4  29/29ed0870.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ecomm_data_final.shape, ecomm_data_final.target_class.nunique())\n",
    "ecomm_data_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14aaf8f-7bb5-4f3e-9fb1-8b390ae1b14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_class\n",
       "Cases & Covers                     3000\n",
       "Shoes                              2878\n",
       "Living Room Furniture              1949\n",
       "Casual Shoes                       1808\n",
       "Men's Shoes                        1753\n",
       "Women                              1364\n",
       "Categories                          946\n",
       "Mobile Accessories                  912\n",
       "Sports & Outdoor Shoes              885\n",
       "Women's Shoes                       691\n",
       "Jewelry                             633\n",
       "Accessories                         622\n",
       "Men                                 581\n",
       "Sandals                             510\n",
       "Tables                              487\n",
       "Sheets & Pillowcases                465\n",
       "Handbags                            383\n",
       "Hats & Caps                         363\n",
       "Kitchen & Dining Room Furniture     363\n",
       "Home Bar Furniture                  362\n",
       "Accent Furniture                    361\n",
       "Lamps & Shades                      355\n",
       "Rings                               349\n",
       "Light Bulbs                         334\n",
       "Earrings                            321\n",
       "Beds, Frames & Bases                307\n",
       "Vitamins                            305\n",
       "Coffee                              287\n",
       "Snack Foods                         283\n",
       "Furniture                           244\n",
       "Pots & Pans                         243\n",
       "Necklaces                           235\n",
       "Door Hardware & Locks               231\n",
       "Headboards & Footboards             227\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_data_final.target_class.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0c19c6-9567-4410-b220-ed8b9e9f9738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name       0\n",
       "hierarchy          0\n",
       "image_id           0\n",
       "clean_hierarchy    0\n",
       "target_class       0\n",
       "path               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_data_final.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da37ae2-94ee-43f8-a049-c40071619849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data point : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_name': 'AmazonBasics 32 Oz Bottle Label, Multi-Purpose Cleaner, Lavender, 5-Pack',\n",
       " 'hierarchy': [{'node_id': 2224149011,\n",
       "   'node_name': '/Categories/Garden Structures & Germination Equipment'}],\n",
       " 'image_id': '51HwfW8BZBL',\n",
       " 'clean_hierarchy': '/Categories/Garden Structures & Germination Equipment',\n",
       " 'target_class': 'Categories',\n",
       " 'path': 'aa/aac0a59b.jpg'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = ecomm_data_final.to_dict(orient = 'records')[randint(0,len(ecomm_data_final))]\n",
    "img = plt.imread(f\"/data/Kishore/Major/abo-images-small/images/small/{sample_data['path']}\")\n",
    "plt.title(f\"{sample_data['product_name'],sample_data['hierarchy'][-1]['node_name']}\")\n",
    "plt.title(\"Sample Data Image\")\n",
    "plt.imshow(img)\n",
    "print(\"Sample Data point : \")\n",
    "sample_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be70930-63c1-4948-b7ce-5beab8d10b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train         17525\n",
      "validation     3756\n",
      "test           3756\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "strat_column = 'target_class'\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "ecomm_data_final['label'] = label_encoder.fit_transform(ecomm_data_final[strat_column])\n",
    "\n",
    "train_data, temp_data = train_test_split(ecomm_data_final,\n",
    "                                         stratify=ecomm_data_final[strat_column],\n",
    "                                         test_size=(val_size + test_size),\n",
    "                                         random_state=42)\n",
    "\n",
    "val_data, test_data = train_test_split(temp_data,\n",
    "                                       stratify=temp_data[strat_column],\n",
    "                                       test_size=test_size/(test_size + val_size),\n",
    "                                       random_state=42)\n",
    "\n",
    "train_data['split'] = 'train'\n",
    "val_data['split'] = 'validation'\n",
    "test_data['split'] = 'test'\n",
    "\n",
    "ecomm_data_final = pd.concat([train_data, val_data, test_data])\n",
    "\n",
    "print(ecomm_data_final['split'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31288834-8a74-47ad-a427-3938d5c806bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 33, 34)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecomm_data_final.to_csv(\"data.csv\",index = False)\n",
    "ecomm_data_final.label.min(),ecomm_data_final.label.max(), ecomm_data_final.target_class.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a641c077-fa47-450f-b248-91332cf21c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = FlavaModel.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9eac77d-957f-46f1-acd2-fb98d18edf32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlavaModel(\n",
       "  (text_model): FlavaTextModel(\n",
       "    (embeddings): FlavaTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (image_model): FlavaImageModel(\n",
       "    (embeddings): FlavaImageEmbeddings(\n",
       "      (patch_embeddings): PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (multimodal_model): FlavaMultimodalModel(\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (image_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (image_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3e00ba0-adb8-47a9-9bb0-72c789f43956",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAVAClassifier(\n",
      "  (image_encoder): FlavaImageModel(\n",
      "    (embeddings): FlavaImageEmbeddings(\n",
      "      (patch_embeddings): PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): FlavaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x FlavaLayer(\n",
      "          (attention): FlavaAttention(\n",
      "            (attention): FlavaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): FlavaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): FlavaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): FlavaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): FlavaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): FlavaTextModel(\n",
      "    (embeddings): FlavaTextEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): FlavaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x FlavaLayer(\n",
      "          (attention): FlavaAttention(\n",
      "            (attention): FlavaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): FlavaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): FlavaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): FlavaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): FlavaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (image_map): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (text_map): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (pre_output): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=65536, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): Linear(in_features=256, out_features=34, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EcommerceDataset(Dataset):\n",
    "    def __init__(self, root_folder = \"/home/jupyter/Kishore/IITJ/Semester_2/Foundational Models/Project\", image_folder = \"/data/Kishore/Major/abo-images-small/images/small/\", split='train', image_size=224):\n",
    "        super(EcommerceDataset, self).__init__()\n",
    "        self.root_folder = root_folder\n",
    "        self.image_folder = image_folder\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        self.info_file = os.path.join(root_folder, 'data.csv')\n",
    "        self.df = pd.read_csv(self.info_file)\n",
    "        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {}\n",
    "        image_path = row['path']\n",
    "        item['image'] = Image.open(f\"{self.image_folder}/{image_path}\").convert('RGB').resize((self.image_size, self.image_size))\n",
    "        item['text'] = row['product_name']\n",
    "        item['labels'] = row['label']\n",
    "        return item\n",
    "    \n",
    "@dataclass\n",
    "class Arguments:\n",
    "    flava_pretrained_model = 'facebook/flava-full'\n",
    "    image_size = 224\n",
    "    batch_size = 32\n",
    "    num_cpus = os.cpu_count()\n",
    "    use_pretrained_map = False\n",
    "    num_mapping_layers = 1\n",
    "    map_dim = 256\n",
    "#     fusion = 'concat'\n",
    "    fusion = 'cross'\n",
    "    num_pre_output_layers = 3\n",
    "    lr = 0.01\n",
    "    weight_decay = 1e-3\n",
    "    weight_image_loss = 0\n",
    "    weight_text_loss = 0\n",
    "    weight_super_loss = 0\n",
    "    drop_probs = [0.1, 0.1, 0.1]\n",
    "    freeze_image_encoder = True\n",
    "    freeze_text_encoder = True\n",
    "    num_class = 34\n",
    "\n",
    "args = Arguments() \n",
    "\n",
    "def load_dataset(args, split):\n",
    "    image_folder = \"/data/Kishore/Major/abo-images-small/images/small/\"\n",
    "    dataset = EcommerceDataset(root_folder='/home/jupyter/Kishore/IITJ/Semester_2/Foundational Models/Project', image_folder=image_folder, split=split, image_size=args.image_size)\n",
    "    return dataset\n",
    "\n",
    "dataset_train = load_dataset(args, split='train')\n",
    "dataset_validation = load_dataset(args, split='validation')\n",
    "dataset_test = load_dataset(args, split='test')\n",
    "\n",
    "class CustomCollator(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        # Load the FLAVA processor for image and text processing\n",
    "        self.processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Extract images and text data from the batch\n",
    "        images = [item['image'] for item in batch]\n",
    "        texts = [item['text'] for item in batch]\n",
    "\n",
    "        # Process images and text using FLAVA processor\n",
    "        processed_inputs = self.processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # Extract necessary tensors\n",
    "        pixel_values = processed_inputs['pixel_values']  # Shape: [batch_size, 3, 224, 224]\n",
    "        input_ids = processed_inputs['input_ids']\n",
    "        attention_mask = processed_inputs['attention_mask']\n",
    "\n",
    "        # Collect labels\n",
    "        labels = torch.LongTensor([item['labels'] for item in batch])\n",
    "\n",
    "        # Build the final batch dictionary\n",
    "        batch_new = {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return batch_new\n",
    "\n",
    "# Initialize the collator with arguments\n",
    "collator = CustomCollator(args)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_cpus, collate_fn=collator)\n",
    "val_dataloader = DataLoader(dataset_validation, batch_size=args.batch_size, shuffle=False, num_workers=args.num_cpus, collate_fn=collator)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=args.batch_size, shuffle=False, num_workers=args.num_cpus, collate_fn=collator)\n",
    "\n",
    "# model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\")\n",
    "# model.classifier = torch.nn.Linear(model.config.hidden_size, len(label_encoder.classes_))\n",
    "\n",
    "# Training setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "class FLAVAClassifier(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_mapping_layers = args.num_mapping_layers\n",
    "        self.map_dim = args.map_dim\n",
    "        self.fusion = args.fusion\n",
    "        self.num_pre_output_layers = args.num_pre_output_layers\n",
    "        self.lr = args.lr\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.weight_image_loss = args.weight_image_loss\n",
    "        self.weight_text_loss = args.weight_text_loss\n",
    "\n",
    "        # Load the FLAVA model\n",
    "        self.flava = FlavaModel.from_pretrained(args.flava_pretrained_model)\n",
    "\n",
    "        \n",
    "        self.image_encoder = copy.deepcopy(self.flava.image_model)\n",
    "        self.text_encoder = copy.deepcopy(self.flava.text_model)\n",
    "\n",
    "        ## Projection Layers\n",
    "        image_map_layers = [nn.Linear(self.image_encoder.config.hidden_size, self.map_dim), nn.Dropout(p=args.drop_probs[0])]\n",
    "        text_map_layers = [nn.Linear(self.text_encoder.config.hidden_size, self.map_dim), nn.Dropout(p=args.drop_probs[0])]\n",
    "        for _ in range(1, self.num_mapping_layers):\n",
    "            image_map_layers.extend([nn.ReLU(), nn.Linear(self.map_dim, self.map_dim), nn.Dropout(p=args.drop_probs[0])])\n",
    "            text_map_layers.extend([nn.ReLU(), nn.Linear(self.map_dim, self.map_dim), nn.Dropout(p=args.drop_probs[0])])\n",
    "\n",
    "        self.image_map = nn.Sequential(*image_map_layers)\n",
    "        self.text_map = nn.Sequential(*text_map_layers)\n",
    "\n",
    "        ## Pre Output Layers\n",
    "        if args.fusion in ['align', 'align_shuffle']:\n",
    "            pre_output_input_dim = self.map_dim\n",
    "        elif args.fusion == 'concat':\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "        elif args.fusion.startswith('cross'):\n",
    "            pre_output_input_dim = self.map_dim**2\n",
    "        elif args.fusion == 'align_concat':\n",
    "            pre_output_input_dim = self.map_dim*3\n",
    "        elif args.fusion == 'attention_m':\n",
    "            self.gen_query = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.gen_key = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.soft = nn.Softmax(dim=1)\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "\n",
    "        pre_output_layers = [nn.Dropout(p=args.drop_probs[1])]\n",
    "        output_input_dim = pre_output_input_dim\n",
    "        if self.num_pre_output_layers >= 1: # first pre-output layer\n",
    "            pre_output_layers.extend([nn.Linear(pre_output_input_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args.drop_probs[2])])\n",
    "            output_input_dim = self.map_dim\n",
    "        for _ in range(1, self.num_pre_output_layers): # next pre-output layers\n",
    "            pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args.drop_probs[2])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        ## Output Layer\n",
    "        self.output = nn.Linear(output_input_dim, args.num_class)\n",
    "\n",
    "        if args.freeze_image_encoder:\n",
    "            for _, p in self.image_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        if args.freeze_text_encoder:\n",
    "            for _, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        del self.flava\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values']).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "#         print(f'image_features: {image_features.shape}')\n",
    "\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "        text_features = self.text_map(text_features)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1) # [batch_size, d]\n",
    "        text_features = F.normalize(text_features, p=2, dim=1) # [batch_size, d]\n",
    "        \n",
    "        print(f'Image features: {image_features.shape}')\n",
    "        print(f'text_features: {text_features.shape}')\n",
    "\n",
    "        if self.fusion in ['align', 'align_shuffle']:\n",
    "            features = torch.mul(image_features, text_features)  # [batch_size, d]\n",
    "\n",
    "        elif self.fusion == 'concat':\n",
    "            features = torch.cat([image_features, text_features], dim=1)  # [batch_size, 2*d]\n",
    "\n",
    "        elif self.fusion.startswith('cross'):\n",
    "            features = torch.bmm(image_features.unsqueeze(2), text_features.unsqueeze(1)) # [batch_size, d, d]\n",
    "            if self.fusion == 'cross_nd':\n",
    "                mask = torch.eye(self.map_dim).repeat(features.shape[0], 1, 1).bool()\n",
    "                features[mask] = torch.zeros(features.shape[0]*self.map_dim, device=features.device)\n",
    "                del mask\n",
    "            features = features.reshape(features.shape[0], -1)  # [batch_size, d*d]\n",
    "\n",
    "        elif self.fusion == 'align_concat':\n",
    "            features = torch.cat([torch.mul(image_features, text_features), image_features, text_features], dim=1)  # [batch_size, 3*d]\n",
    "\n",
    "        elif self.fusion == 'attention_m':\n",
    "            q1 = F.relu(self.gen_query(image_features))\n",
    "            k1 = F.relu(self.gen_key(image_features))\n",
    "            q2 = F.relu(self.gen_query(text_features))\n",
    "            k2 = F.relu(self.gen_key(text_features))\n",
    "            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n",
    "            prob_1 = wt_i1_i2[:,0]\n",
    "            prob_2 = wt_i1_i2[:,1]\n",
    "            wtd_i1 = image_features * prob_1[:, None]\n",
    "            wtd_i2 = text_features * prob_2[:, None]\n",
    "            features = torch.cat((wtd_i1,wtd_i2), 1) # [batch_size, 2*d]\n",
    "        else:\n",
    "                raise ValueError()\n",
    "\n",
    "        features = self.pre_output(features)\n",
    "        logits = self.output(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = FLAVAClassifier(args)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def train_and_validate(model, train_dataloader, val_dataloader, epochs, device, loss_fn, optimizer, scheduler=None, print_every=100):\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "        train_progress = tqdm(train_dataloader, desc='Training', leave=False)\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_progress):\n",
    "            # Move batch to device\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model({\n",
    "                'pixel_values': pixel_values,\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            })\n",
    "\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(preds, dim=1).detach().cpu().numpy())\n",
    "            train_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            if batch_idx % print_every == 0:\n",
    "                avg_loss = total_train_loss / (batch_idx + 1)\n",
    "                print(f\"Batch {batch_idx}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "        print(f\"Epoch {epoch + 1} Training Loss: {total_train_loss / len(train_dataloader):.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(val_dataloader, desc='Validating', leave=False)\n",
    "\n",
    "            for batch in val_progress:\n",
    "\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                preds = model({\n",
    "                    'pixel_values': pixel_values,\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                })\n",
    "\n",
    "                loss = loss_fn(preds, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                val_preds.extend(torch.argmax(preds, dim=1).detach().cpu().numpy())\n",
    "                val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        print(f\"Epoch {epoch + 1} Validation Loss: {total_val_loss / len(val_dataloader):.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"Best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145ab44-9302-449d-9c9e-11fe9bcae6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "004af05d-3c70-454e-9009-4b915b9a7112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<08:28,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 3.5212\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:52,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:52,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:33,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:02<01:22,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:19,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:17,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:16,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:14,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:13,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:04<01:14,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:13,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:12,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:15,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:13,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:13,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:11,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:11,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:10,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:10,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:09,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:10,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:09,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:08,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:09<01:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:08,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:07,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:07,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:07,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:11<01:07,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:06,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:06,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:05,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:04,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:13<01:04,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:05,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:03,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:03,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:04,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 100, Training Loss: 3.2680\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:01,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:01,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:01,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<00:59,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<00:59,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<00:59,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<00:59,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:58,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<01:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:59,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:58,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:20<00:57,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:58,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:57,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:56,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:56,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:56,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:55,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:22<00:55,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:55,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:55,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:54,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:53,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:53,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:23<00:53,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:53,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:53,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:53,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:52,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:51,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:51,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:51,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:51,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:51,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:51,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:51,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:50,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:50,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:49,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:49,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:48,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:50,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:49,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 200, Training Loss: 3.2231\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:48,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:48,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:47,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:46,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:46,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:46,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:30<00:46,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:46,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:46,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:46,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:45,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:44,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:45,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:32<00:44,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:44,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:43,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:43,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:43,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:42,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:43,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:34<00:44,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:42,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:42,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:42,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:41,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:41,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:41,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:40,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:40,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:39,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:39,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:39,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:38,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:39,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:38,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:38,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:39<00:37,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:37,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:37,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:37,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:36,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:36,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:36,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:41<00:36,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:35,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:34,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 300, Training Loss: 3.2121\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:34,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:34,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:34,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:33,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:32,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:32,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:31,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:31,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:31,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:30,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:30,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:29,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:29,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:48<00:28,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:28,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:27,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:27,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:26,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:50<00:26,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:26,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:25,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:26,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:25,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:52<00:24,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:23,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:23,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:22,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 400, Training Loss: 3.2022\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:20,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:57<00:19,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:18,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:18,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [00:59<00:17,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:16,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:17,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:01<00:15,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:15,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:15,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:13,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:13,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:12,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:10,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:10,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:08<00:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:08,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:10<00:06,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 500, Training Loss: 3.1954\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:06,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:12<00:04,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 3.1970\n",
      "Epoch 1 Training Accuracy: 0.1178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:43,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:34,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:22,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:18,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:01<00:16,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:12,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:06<00:10,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:10,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 63/118 [00:09<00:07,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  55%|█████▌    | 65/118 [00:09<00:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  57%|█████▋    | 67/118 [00:10<00:07,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 69/118 [00:10<00:06,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  60%|██████    | 71/118 [00:10<00:06,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▏   | 73/118 [00:10<00:06,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▎   | 75/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  65%|██████▌   | 77/118 [00:11<00:05,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 79/118 [00:11<00:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▊   | 81/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 83/118 [00:12<00:04,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  72%|███████▏  | 85/118 [00:12<00:04,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 87/118 [00:12<00:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 89/118 [00:13<00:04,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  77%|███████▋  | 91/118 [00:13<00:03,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 93/118 [00:13<00:03,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 95/118 [00:13<00:03,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  82%|████████▏ | 97/118 [00:14<00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 99/118 [00:14<00:02,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▌ | 101/118 [00:14<00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  87%|████████▋ | 103/118 [00:15<00:02,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 105/118 [00:15<00:01,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  91%|█████████ | 107/118 [00:15<00:01,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 109/118 [00:15<00:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  94%|█████████▍| 111/118 [00:16<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  96%|█████████▌| 113/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 115/118 [00:16<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  99%|█████████▉| 117/118 [00:16<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([12, 256])\n",
      "text_features: torch.Size([12, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 3.1985\n",
      "Epoch 1 Validation Accuracy: 0.1198\n",
      "Best model saved with validation accuracy: 0.1198\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<08:14,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 3.2753\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:53,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:53,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:33,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:02<01:24,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:19,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:16,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:15,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:14,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:13,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:13,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:04<01:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:13,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:12,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:12,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:13,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:12,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:12,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:11,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:11,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:11,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:11,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:10,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:10,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:09,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:09,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:09<01:09,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:08,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:07,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:07,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:07,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:06,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:11<01:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:06,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:13<01:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:04,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:05,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:03,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:03,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 100, Training Loss: 3.1745\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:03,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:03,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:02,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:02,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:01,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:01,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<01:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<00:59,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<01:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<00:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:58,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:59,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<00:59,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:58,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:57,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:20<00:57,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:57,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:57,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:56,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:56,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:55,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:57,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:22<00:56,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:55,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:55,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:55,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:54,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:54,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:24<00:54,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:54,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:52,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:52,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:52,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:52,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:52,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:51,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:51,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:50,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:50,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:50,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:50,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:49,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:50,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:50,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:49,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:49,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:48,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 200, Training Loss: 3.1794\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:48,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:47,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:47,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:48,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:48,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:47,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:31<00:47,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:47,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:46,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:46,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:45,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:45,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:44,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:33<00:44,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:44,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:44,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:43,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:43,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:44,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:44,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:34<00:43,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:43,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:42,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:42,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:41,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:41,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:41,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:40,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:40,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:39,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:39,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:38,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:38,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:38,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:37,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:37,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:40<00:37,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:37,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:37,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:37,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:37,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:36,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:36,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:42<00:36,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:36,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:35,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 300, Training Loss: 3.1808\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:34,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:33,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:33,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:32,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:32,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:32,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:31,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:31,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:31,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:30,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:30,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:30,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:29,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:49<00:28,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:28,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:28,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:27,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:27,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:51<00:26,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:26,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:25,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:25,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:25,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:52<00:25,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:23,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:23,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:23,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 400, Training Loss: 3.1865\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:20,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:58<00:19,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:18,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:18,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [01:00<00:17,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:17,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:16,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:02<00:16,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:14,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:14,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:14,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:13,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:12,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:10,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:09<00:08,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:11<00:06,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 500, Training Loss: 3.1841\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:12<00:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 3.1834\n",
      "Epoch 2 Training Accuracy: 0.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:54,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:37,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:23,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:19,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:02<00:16,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:12,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:07<00:10,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 63/118 [00:09<00:07,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  55%|█████▌    | 65/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  57%|█████▋    | 67/118 [00:10<00:07,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 69/118 [00:10<00:06,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  60%|██████    | 71/118 [00:10<00:06,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▏   | 73/118 [00:10<00:06,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▎   | 75/118 [00:11<00:05,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  65%|██████▌   | 77/118 [00:11<00:05,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 79/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▊   | 81/118 [00:12<00:05,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 83/118 [00:12<00:04,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  72%|███████▏  | 85/118 [00:12<00:04,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 87/118 [00:12<00:04,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 89/118 [00:13<00:04,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  77%|███████▋  | 91/118 [00:13<00:03,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 93/118 [00:13<00:03,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 95/118 [00:13<00:03,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  82%|████████▏ | 97/118 [00:14<00:02,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 99/118 [00:14<00:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▌ | 101/118 [00:14<00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  87%|████████▋ | 103/118 [00:15<00:02,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 105/118 [00:15<00:01,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  91%|█████████ | 107/118 [00:15<00:01,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 109/118 [00:15<00:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  94%|█████████▍| 111/118 [00:16<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  96%|█████████▌| 113/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 115/118 [00:16<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  99%|█████████▉| 117/118 [00:16<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([12, 256])\n",
      "text_features: torch.Size([12, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 3.1805\n",
      "Epoch 2 Validation Accuracy: 0.1147\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<08:08,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 3.2048\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:48,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:52,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:33,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:02<01:23,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:18,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:16,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:15,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:15,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:15,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:14,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:03<01:13,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:13,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:12,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:12,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:12,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:12,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:12,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:11,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:11,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:10,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:10,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:10,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:09,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:08,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:09<01:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:08,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:08,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:07,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:07,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:06,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:11<01:06,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:06,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:05,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:05,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:05,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:12<01:06,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:05,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:04,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:05,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:03,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:03,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:02,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 100, Training Loss: 3.1692\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:03,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:02,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:02,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:01,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:01,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<01:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<00:59,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<00:59,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<00:59,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:58,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:59,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<00:58,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:58,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:57,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:20<00:57,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:58,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:58,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:57,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:57,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:56,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:55,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:22<00:55,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:56,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:55,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:54,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:54,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:54,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:24<00:54,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:53,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:52,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:52,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:53,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:52,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:52,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:51,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:51,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:51,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:51,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:51,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:51,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:50,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:49,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:50,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:49,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:49,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:49,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 200, Training Loss: 3.1837\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:48,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:47,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:47,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:47,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:48,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:47,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:31<00:46,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:46,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:46,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:46,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:45,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:44,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:44,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:33<00:44,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:44,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:43,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:43,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:43,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:43,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:44,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:35<00:43,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:42,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:42,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:41,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:41,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:41,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:40,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:40,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:40,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:40,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:39,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:39,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:38,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:38,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:37,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:37,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:40<00:37,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:36,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:36,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:36,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:36,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:36,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:36,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:42<00:36,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:35,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:34,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:34,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 300, Training Loss: 3.1825\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:33,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:34,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:33,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:33,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:34,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:33,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:32,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:32,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:32,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:30,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:30,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:30,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:29,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:49<00:28,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:28,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:28,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:28,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:26,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:51<00:26,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:26,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:25,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:25,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:24,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:53<00:24,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:23,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:23,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:22,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 400, Training Loss: 3.1855\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:20,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:58<00:19,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:19,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:18,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [01:00<00:17,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:17,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:17,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:02<00:15,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:15,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:14,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:13,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:13,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:12,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:11,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:09<00:08,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:08,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:11<00:06,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 500, Training Loss: 3.1840\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:12<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 3.1813\n",
      "Epoch 3 Training Accuracy: 0.1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:45,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:35,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:23,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:18,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:02<00:16,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:06<00:10,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:09,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:08,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 63/118 [00:09<00:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  55%|█████▌    | 65/118 [00:09<00:07,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  57%|█████▋    | 67/118 [00:10<00:07,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 69/118 [00:10<00:06,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  60%|██████    | 71/118 [00:10<00:06,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▏   | 73/118 [00:10<00:06,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▎   | 75/118 [00:11<00:05,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  65%|██████▌   | 77/118 [00:11<00:05,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 79/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▊   | 81/118 [00:11<00:05,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 83/118 [00:12<00:04,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  72%|███████▏  | 85/118 [00:12<00:04,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 87/118 [00:12<00:04,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 89/118 [00:13<00:04,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  77%|███████▋  | 91/118 [00:13<00:03,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 93/118 [00:13<00:03,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 95/118 [00:13<00:03,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  82%|████████▏ | 97/118 [00:14<00:02,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 99/118 [00:14<00:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▌ | 101/118 [00:14<00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  87%|████████▋ | 103/118 [00:15<00:02,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 105/118 [00:15<00:01,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  91%|█████████ | 107/118 [00:15<00:01,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 109/118 [00:15<00:01,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  94%|█████████▍| 111/118 [00:16<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  96%|█████████▌| 113/118 [00:16<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 114/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 115/118 [00:16<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  98%|█████████▊| 116/118 [00:16<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  99%|█████████▉| 117/118 [00:16<00:00,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([12, 256])\n",
      "text_features: torch.Size([12, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 3.1751\n",
      "Epoch 3 Validation Accuracy: 0.1198\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<07:44,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 2.9715\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:42,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:49,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:30,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:01<01:23,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:18,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:17,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:15,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:14,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:16,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:15,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 22/548 [00:03<01:14,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:03<01:14,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 24/548 [00:04<01:13,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:13,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 26/548 [00:04<01:12,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:12,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 28/548 [00:04<01:12,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:12,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 30/548 [00:04<01:11,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:12,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 32/548 [00:05<01:12,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:11,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 34/548 [00:05<01:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:12,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 36/548 [00:05<01:12,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:11,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 38/548 [00:06<01:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:10,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 40/548 [00:06<01:10,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:10,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 42/548 [00:06<01:10,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 44/548 [00:06<01:10,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:10,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 46/548 [00:07<01:10,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 48/548 [00:07<01:10,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:09,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 50/548 [00:07<01:10,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:09,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 52/548 [00:08<01:08,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:08,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 54/548 [00:08<01:09,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:08,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 56/548 [00:08<01:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:09,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 58/548 [00:08<01:09,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:08<01:08,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 60/548 [00:09<01:08,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:08,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 62/548 [00:09<01:07,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:07,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 64/548 [00:09<01:07,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:08,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 66/548 [00:09<01:08,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 68/548 [00:10<01:07,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:08,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 70/548 [00:10<01:07,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:06,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 72/548 [00:10<01:07,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:10<01:07,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 74/548 [00:11<01:06,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:07,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 76/548 [00:11<01:07,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:07,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 78/548 [00:11<01:06,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:06,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 80/548 [00:11<01:06,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:05,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 82/548 [00:12<01:05,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:05,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 84/548 [00:12<01:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:05,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 86/548 [00:12<01:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:12<01:04,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 88/548 [00:13<01:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:04,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▋        | 90/548 [00:13<01:03,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:03,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 92/548 [00:13<01:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:03,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 94/548 [00:13<01:05,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:04,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 96/548 [00:14<01:04,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 98/548 [00:14<01:03,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:09,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 100/548 [00:14<01:08,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:06,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Training Loss: 3.1634\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 102/548 [00:15<01:04,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:03,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 104/548 [00:15<01:02,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:02,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 106/548 [00:15<01:02,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:02,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 108/548 [00:15<01:02,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:01,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 110/548 [00:16<01:01,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:01,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 112/548 [00:16<01:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 114/548 [00:16<01:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 116/548 [00:17<01:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 118/548 [00:17<01:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<01:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 120/548 [00:17<00:59,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<00:59,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 122/548 [00:17<00:59,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<00:59,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 124/548 [00:18<00:59,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<00:59,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 126/548 [00:18<00:58,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:59,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 128/548 [00:18<00:59,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:59,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 130/548 [00:19<00:59,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<00:59,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 132/548 [00:19<00:59,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:58,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 134/548 [00:19<00:58,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:57,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 136/548 [00:19<00:57,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:20<00:57,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 138/548 [00:20<00:57,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:57,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 140/548 [00:20<00:57,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:58,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 142/548 [00:20<00:57,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:57,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 144/548 [00:21<00:56,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:56,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 146/548 [00:21<00:56,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:55,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 148/548 [00:21<00:55,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:55,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 150/548 [00:21<00:55,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:21<00:55,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 152/548 [00:22<00:55,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:55,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 154/548 [00:22<00:55,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:55,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 156/548 [00:22<00:54,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:55,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 158/548 [00:22<00:55,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 160/548 [00:23<00:55,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:55,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 162/548 [00:23<00:55,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:54,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 164/548 [00:23<00:54,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:23<00:54,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 166/548 [00:24<00:54,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:54,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 168/548 [00:24<00:53,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:53,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 170/548 [00:24<00:52,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:53,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███▏      | 172/548 [00:24<00:53,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:52,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 174/548 [00:25<00:52,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:52,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 176/548 [00:25<00:52,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:52,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 178/548 [00:25<00:52,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:51,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 180/548 [00:26<00:51,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:51,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 182/548 [00:26<00:51,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:51,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▎      | 184/548 [00:26<00:50,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:50,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 186/548 [00:26<00:50,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:50,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 188/548 [00:27<00:51,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:50,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 190/548 [00:27<00:50,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:50,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 192/548 [00:27<00:49,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:49,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 194/548 [00:28<00:49,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:49,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 196/548 [00:28<00:49,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:49,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 198/548 [00:28<00:49,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:49,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 200/548 [00:28<00:49,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:49,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Training Loss: 3.1664\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 202/548 [00:29<00:49,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:49,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 204/548 [00:29<00:49,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:49,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 206/548 [00:29<00:48,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:48,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 208/548 [00:30<00:47,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:47,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 210/548 [00:30<00:47,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:47,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 212/548 [00:30<00:46,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:46,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 214/548 [00:30<00:47,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:31<00:47,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 216/548 [00:31<00:47,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:46,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 218/548 [00:31<00:46,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:45,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 220/548 [00:31<00:45,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:46,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 222/548 [00:31<00:45,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:45,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 224/548 [00:32<00:45,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:44,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 226/548 [00:32<00:45,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:45,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 228/548 [00:32<00:45,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:32<00:45,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 230/548 [00:33<00:44,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:44,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 232/548 [00:33<00:44,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:43,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 234/548 [00:33<00:43,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:44,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 236/548 [00:33<00:44,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:44,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 238/548 [00:34<00:43,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:43,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 240/548 [00:34<00:43,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:43,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 242/548 [00:34<00:42,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:34<00:42,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 244/548 [00:35<00:42,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:42,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 246/548 [00:35<00:42,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:41,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 248/548 [00:35<00:41,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:42,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 250/548 [00:35<00:41,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:41,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 252/548 [00:36<00:41,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 254/548 [00:36<00:40,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:40,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 256/548 [00:36<00:40,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:40,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 258/548 [00:37<00:40,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:40,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 260/548 [00:37<00:40,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:40,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 262/548 [00:37<00:40,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:40,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 264/548 [00:37<00:40,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:40,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 266/548 [00:38<00:39,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 268/548 [00:38<00:39,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:38,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 270/548 [00:38<00:39,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:39,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 272/548 [00:39<00:38,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:39,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 274/548 [00:39<00:38,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:38,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 276/548 [00:39<00:38,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:37,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 278/548 [00:39<00:37,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:39<00:37,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 280/548 [00:40<00:37,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:36,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 282/548 [00:40<00:37,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:37,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 284/548 [00:40<00:37,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:37,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 286/548 [00:40<00:37,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:37,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 288/548 [00:41<00:36,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:36,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 290/548 [00:41<00:36,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:35,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 292/548 [00:41<00:35,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:41<00:36,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▎    | 294/548 [00:42<00:36,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:35,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 296/548 [00:42<00:35,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 298/548 [00:42<00:35,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:35,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 300/548 [00:42<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:34,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Training Loss: 3.1735\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 302/548 [00:43<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:34,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 304/548 [00:43<00:35,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:34,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 306/548 [00:43<00:34,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:33,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 308/548 [00:44<00:33,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 310/548 [00:44<00:33,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 312/548 [00:44<00:32,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:32,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 314/548 [00:44<00:32,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:32,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 316/548 [00:45<00:32,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:32,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 318/548 [00:45<00:32,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:32,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 320/548 [00:45<00:32,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:31,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 322/548 [00:46<00:31,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:31,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 324/548 [00:46<00:31,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 326/548 [00:46<00:31,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:30,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 328/548 [00:46<00:31,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:31,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 330/548 [00:47<00:30,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 332/548 [00:47<00:30,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 334/548 [00:47<00:29,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:30,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 336/548 [00:47<00:29,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 338/548 [00:48<00:29,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 340/548 [00:48<00:29,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:29,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 342/548 [00:48<00:29,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:48<00:28,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 344/548 [00:49<00:28,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:28,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 346/548 [00:49<00:28,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 348/548 [00:49<00:27,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:27,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 350/548 [00:49<00:27,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:27,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 352/548 [00:50<00:27,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 354/548 [00:50<00:27,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:27,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 356/548 [00:50<00:26,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:50<00:26,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 358/548 [00:51<00:26,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 360/548 [00:51<00:26,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:26,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 362/548 [00:51<00:26,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:26,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▋   | 364/548 [00:51<00:26,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:26,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 366/548 [00:52<00:25,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 368/548 [00:52<00:25,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:25,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 370/548 [00:52<00:24,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:52<00:24,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 372/548 [00:53<00:24,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 374/548 [00:53<00:24,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▊   | 376/548 [00:53<00:23,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:23,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 378/548 [00:53<00:23,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:24,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 380/548 [00:54<00:23,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 382/548 [00:54<00:23,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 384/548 [00:54<00:23,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:23,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 386/548 [00:55<00:23,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 388/548 [00:55<00:22,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 390/548 [00:55<00:22,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 392/548 [00:55<00:22,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 394/548 [00:56<00:21,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 396/548 [00:56<00:21,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 398/548 [00:56<00:21,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 400/548 [00:57<00:20,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Training Loss: 3.1809\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 402/548 [00:57<00:20,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 404/548 [00:57<00:20,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:20,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 406/548 [00:57<00:20,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:58<00:20,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 408/548 [00:58<00:19,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 410/548 [00:58<00:19,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 412/548 [00:58<00:19,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:18,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 414/548 [00:58<00:18,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 416/548 [00:59<00:18,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 418/548 [00:59<00:18,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:17,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 420/548 [00:59<00:17,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [00:59<00:17,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 422/548 [01:00<00:17,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 424/548 [01:00<00:17,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:17,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 426/548 [01:00<00:17,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:16,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 428/548 [01:00<00:16,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 430/548 [01:01<00:16,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 432/548 [01:01<00:16,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 434/548 [01:01<00:16,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:01<00:15,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 436/548 [01:02<00:15,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 438/548 [01:02<00:15,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 440/548 [01:02<00:15,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:15,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 442/548 [01:02<00:14,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:14,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 444/548 [01:03<00:14,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████▏ | 446/548 [01:03<00:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 448/548 [01:03<00:14,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:14,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 450/548 [01:04<00:14,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:14,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 452/548 [01:04<00:13,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 454/548 [01:04<00:13,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 456/548 [01:04<00:13,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:13,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▎ | 458/548 [01:05<00:12,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 460/548 [01:05<00:12,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 462/548 [01:05<00:12,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 464/548 [01:06<00:11,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 466/548 [01:06<00:11,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 468/548 [01:06<00:11,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 470/548 [01:06<00:10,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:10,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 472/548 [01:07<00:10,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 474/548 [01:07<00:10,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 476/548 [01:07<00:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:09,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 478/548 [01:08<00:09,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 480/548 [01:08<00:09,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 482/548 [01:08<00:09,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 484/548 [01:08<00:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:08<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 486/548 [01:09<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 488/548 [01:09<00:08,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 490/548 [01:09<00:08,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:08,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 492/548 [01:09<00:07,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 494/548 [01:10<00:07,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 496/548 [01:10<00:07,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 498/548 [01:10<00:07,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:10<00:06,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 500/548 [01:11<00:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Training Loss: 3.1794\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 502/548 [01:11<00:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 504/548 [01:11<00:06,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:06,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 506/548 [01:11<00:05,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 508/548 [01:12<00:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 510/548 [01:12<00:05,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 512/548 [01:12<00:05,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:12<00:04,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 514/548 [01:13<00:04,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 516/548 [01:13<00:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 518/548 [01:13<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 520/548 [01:13<00:03,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 522/548 [01:14<00:03,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 524/548 [01:14<00:03,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 526/548 [01:14<00:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▋| 528/548 [01:15<00:02,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 530/548 [01:15<00:02,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 532/548 [01:15<00:02,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 534/548 [01:15<00:02,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 536/548 [01:16<00:01,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 538/548 [01:16<00:01,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 540/548 [01:16<00:01,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 542/548 [01:17<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 544/548 [01:17<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 546/548 [01:17<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 3.1788\n",
      "Epoch 4 Training Accuracy: 0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:34,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:23,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:18,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:01<00:16,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:12,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:06<00:10,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:09,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:08,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 63/118 [00:09<00:07,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  55%|█████▌    | 65/118 [00:09<00:07,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  57%|█████▋    | 67/118 [00:09<00:07,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 69/118 [00:10<00:06,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  60%|██████    | 71/118 [00:10<00:06,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▏   | 73/118 [00:10<00:06,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▎   | 75/118 [00:11<00:05,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  65%|██████▌   | 77/118 [00:11<00:05,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 79/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▊   | 81/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 83/118 [00:12<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  72%|███████▏  | 85/118 [00:12<00:04,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 87/118 [00:12<00:04,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 89/118 [00:13<00:04,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  77%|███████▋  | 91/118 [00:13<00:03,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 93/118 [00:13<00:03,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 95/118 [00:13<00:03,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  82%|████████▏ | 97/118 [00:14<00:03,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 99/118 [00:14<00:02,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▌ | 101/118 [00:14<00:02,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  87%|████████▋ | 103/118 [00:14<00:02,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 105/118 [00:15<00:01,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  91%|█████████ | 107/118 [00:15<00:01,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 109/118 [00:15<00:01,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  94%|█████████▍| 111/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  96%|█████████▌| 113/118 [00:16<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 115/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  99%|█████████▉| 117/118 [00:16<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([12, 256])\n",
      "text_features: torch.Size([12, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 3.1752\n",
      "Epoch 4 Validation Accuracy: 0.1198\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<08:19,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 2.9831\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:51,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:53,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:33,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:02<01:23,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:18,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:17,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:16,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:15,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:15,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:04<01:13,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:14,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:15,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:13,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:13,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:13,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:11,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:10,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:10,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:09,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:09,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:11,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:10,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:10,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:10,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:09,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:09,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:09<01:08,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:07,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:07,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:06,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:06,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:06,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:11<01:05,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:06,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:06,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:13<01:05,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:05,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:04,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:03,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:03,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:03,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:02,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 100, Training Loss: 3.1609\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:02,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:01,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:01,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:01,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:02,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:02,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:01,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:01,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<01:01,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<00:59,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<00:59,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<00:58,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:59,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:58,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<00:58,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:57,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:57,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:20<00:57,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:58,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:57,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:57,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:56,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:55,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:56,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:21<00:55,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:55,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:54,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:54,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:54,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:53,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:23<00:54,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:53,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:52,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:53,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:53,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:52,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:52,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:52,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:51,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:50,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:50,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:50,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:50,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:50,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:50,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:49,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:49,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:49,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:49,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 200, Training Loss: 3.1736\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:48,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:48,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:47,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:47,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:47,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:46,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:31<00:46,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:46,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:46,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:46,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:46,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:45,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:45,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:32<00:45,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:45,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:44,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:43,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:43,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:43,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:43,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:34<00:42,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:42,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:42,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:42,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:42,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:41,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:40,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:40,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:39,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:39,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:39,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:39,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:38,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:38,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:38,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:37,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:40<00:37,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:37,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:36,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:36,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:36,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:36,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:36,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:41<00:35,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:35,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:35,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:34,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 300, Training Loss: 3.1797\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:34,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:34,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:34,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:33,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:33,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:32,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:32,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:32,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:31,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:31,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:30,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:30,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:28,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:49<00:29,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:29,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:28,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:27,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:26,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:51<00:26,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:25,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:25,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:25,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:25,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:52<00:25,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:24,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:24,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:22,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 400, Training Loss: 3.1821\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:19,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:58<00:19,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:18,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:18,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [00:59<00:17,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:17,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:16,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:01<00:15,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:14,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:14,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:14,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:13,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:12,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:12,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:10,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:09,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:09<00:08,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:08,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:10<00:06,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 500, Training Loss: 3.1780\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:06,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 512/548 [01:12<00:05,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:12<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 514/548 [01:13<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 516/548 [01:13<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 518/548 [01:13<00:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 520/548 [01:13<00:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 522/548 [01:14<00:03,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 524/548 [01:14<00:03,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 526/548 [01:14<00:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▋| 528/548 [01:15<00:02,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 530/548 [01:15<00:02,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 532/548 [01:15<00:02,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 534/548 [01:15<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 536/548 [01:16<00:01,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 538/548 [01:16<00:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 540/548 [01:16<00:01,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 542/548 [01:17<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 544/548 [01:17<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 546/548 [01:17<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 3.1776\n",
      "Epoch 5 Training Accuracy: 0.1187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:50,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:36,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:23,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:18,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:02<00:16,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  20%|██        | 24/118 [00:04<00:13,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  22%|██▏       | 26/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  24%|██▎       | 28/118 [00:04<00:12,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▌       | 30/118 [00:04<00:12,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:12,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  27%|██▋       | 32/118 [00:05<00:11,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  29%|██▉       | 34/118 [00:05<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███       | 36/118 [00:05<00:11,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  32%|███▏      | 38/118 [00:06<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  34%|███▍      | 40/118 [00:06<00:10,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▌      | 42/118 [00:06<00:10,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  37%|███▋      | 44/118 [00:06<00:10,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:06<00:10,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  39%|███▉      | 46/118 [00:07<00:09,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  41%|████      | 48/118 [00:07<00:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 50/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  44%|████▍     | 52/118 [00:07<00:09,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:09,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  46%|████▌     | 54/118 [00:08<00:08,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 56/118 [00:08<00:08,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  49%|████▉     | 58/118 [00:08<00:08,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  51%|█████     | 60/118 [00:09<00:08,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 62/118 [00:09<00:07,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 63/118 [00:09<00:07,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  54%|█████▍    | 64/118 [00:09<00:07,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  55%|█████▌    | 65/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  56%|█████▌    | 66/118 [00:09<00:07,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  57%|█████▋    | 67/118 [00:10<00:07,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 68/118 [00:10<00:06,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 69/118 [00:10<00:06,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  59%|█████▉    | 70/118 [00:10<00:06,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  60%|██████    | 71/118 [00:10<00:06,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  61%|██████    | 72/118 [00:10<00:06,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▏   | 73/118 [00:10<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  63%|██████▎   | 74/118 [00:11<00:06,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▎   | 75/118 [00:11<00:05,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  64%|██████▍   | 76/118 [00:11<00:05,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  65%|██████▌   | 77/118 [00:11<00:05,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  66%|██████▌   | 78/118 [00:11<00:05,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 79/118 [00:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  68%|██████▊   | 80/118 [00:11<00:05,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▊   | 81/118 [00:11<00:05,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  69%|██████▉   | 82/118 [00:12<00:05,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  70%|███████   | 83/118 [00:12<00:04,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  71%|███████   | 84/118 [00:12<00:04,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  72%|███████▏  | 85/118 [00:12<00:04,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  73%|███████▎  | 86/118 [00:12<00:04,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 87/118 [00:12<00:04,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▍  | 88/118 [00:12<00:04,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 89/118 [00:13<00:04,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  76%|███████▋  | 90/118 [00:13<00:03,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  77%|███████▋  | 91/118 [00:13<00:03,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  78%|███████▊  | 92/118 [00:13<00:03,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 93/118 [00:13<00:03,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  80%|███████▉  | 94/118 [00:13<00:03,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 95/118 [00:13<00:03,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████▏ | 96/118 [00:14<00:03,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  82%|████████▏ | 97/118 [00:14<00:02,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  83%|████████▎ | 98/118 [00:14<00:02,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 99/118 [00:14<00:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  85%|████████▍ | 100/118 [00:14<00:02,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▌ | 101/118 [00:14<00:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  86%|████████▋ | 102/118 [00:14<00:02,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  87%|████████▋ | 103/118 [00:15<00:02,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  88%|████████▊ | 104/118 [00:15<00:01,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 105/118 [00:15<00:01,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  90%|████████▉ | 106/118 [00:15<00:01,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  91%|█████████ | 107/118 [00:15<00:01,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 108/118 [00:15<00:01,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  92%|█████████▏| 109/118 [00:15<00:01,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  93%|█████████▎| 110/118 [00:16<00:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  94%|█████████▍| 111/118 [00:16<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 112/118 [00:16<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  96%|█████████▌| 113/118 [00:16<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 114/118 [00:16<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|█████████▋| 115/118 [00:16<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  98%|█████████▊| 116/118 [00:16<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  99%|█████████▉| 117/118 [00:16<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([12, 256])\n",
      "text_features: torch.Size([12, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss: 3.1734\n",
      "Epoch 5 Validation Accuracy: 0.1147\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/548 [00:00<07:57,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 0, Training Loss: 2.9089\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/548 [00:01<02:46,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/548 [00:01<01:49,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 7/548 [00:01<01:31,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/548 [00:02<01:24,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/548 [00:02<01:18,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 13/548 [00:02<01:17,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/548 [00:02<01:15,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 17/548 [00:03<01:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 19/548 [00:03<01:14,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/548 [00:03<01:14,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 23/548 [00:03<01:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 25/548 [00:04<01:14,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 27/548 [00:04<01:13,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 29/548 [00:04<01:12,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 31/548 [00:05<01:13,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 33/548 [00:05<01:12,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 35/548 [00:05<01:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 37/548 [00:05<01:11,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 39/548 [00:06<01:11,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 41/548 [00:06<01:11,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 43/548 [00:06<01:10,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 45/548 [00:07<01:11,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 47/548 [00:07<01:10,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 49/548 [00:07<01:10,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 51/548 [00:07<01:10,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 53/548 [00:08<01:08,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 55/548 [00:08<01:09,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 57/548 [00:08<01:08,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 59/548 [00:09<01:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 61/548 [00:09<01:07,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 63/548 [00:09<01:07,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 65/548 [00:09<01:07,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 67/548 [00:10<01:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 69/548 [00:10<01:08,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 71/548 [00:10<01:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 73/548 [00:10<01:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 75/548 [00:11<01:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 77/548 [00:11<01:06,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 79/548 [00:11<01:06,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 81/548 [00:12<01:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 83/548 [00:12<01:05,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 85/548 [00:12<01:06,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 87/548 [00:12<01:04,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 89/548 [00:13<01:04,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 91/548 [00:13<01:03,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 93/548 [00:13<01:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 95/548 [00:14<01:03,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 97/548 [00:14<01:03,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 99/548 [00:14<01:02,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 101/548 [00:14<01:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 100, Training Loss: 3.1680\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 103/548 [00:15<01:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 105/548 [00:15<01:02,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 107/548 [00:15<01:01,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 109/548 [00:16<01:01,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 111/548 [00:16<01:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 113/548 [00:16<01:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 115/548 [00:16<01:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 117/548 [00:17<01:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 119/548 [00:17<00:59,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 121/548 [00:17<01:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 123/548 [00:18<01:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 125/548 [00:18<01:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 127/548 [00:18<00:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 129/548 [00:18<00:59,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 131/548 [00:19<00:59,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 133/548 [00:19<00:58,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 135/548 [00:19<00:58,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 137/548 [00:19<00:57,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 139/548 [00:20<00:58,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 141/548 [00:20<00:58,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 143/548 [00:20<00:57,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 145/548 [00:21<00:57,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 147/548 [00:21<00:56,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 149/548 [00:21<00:56,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 151/548 [00:21<00:55,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 153/548 [00:22<00:55,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 155/548 [00:22<00:54,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 157/548 [00:22<00:54,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 159/548 [00:23<00:54,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 161/548 [00:23<00:56,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 163/548 [00:23<00:55,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 165/548 [00:23<00:54,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 167/548 [00:24<00:53,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 169/548 [00:24<00:53,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 171/548 [00:24<00:53,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 173/548 [00:25<00:52,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 175/548 [00:25<00:52,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 177/548 [00:25<00:52,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 179/548 [00:25<00:51,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 181/548 [00:26<00:52,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 183/548 [00:26<00:51,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 185/548 [00:26<00:51,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 187/548 [00:27<00:50,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 189/548 [00:27<00:50,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 191/548 [00:27<00:50,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 193/548 [00:27<00:50,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 195/548 [00:28<00:49,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 197/548 [00:28<00:48,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 199/548 [00:28<00:49,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 201/548 [00:29<00:48,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 200, Training Loss: 3.1773\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 203/548 [00:29<00:48,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 205/548 [00:29<00:48,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 207/548 [00:29<00:48,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 209/548 [00:30<00:47,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 211/548 [00:30<00:47,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 213/548 [00:30<00:47,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 215/548 [00:31<00:47,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 217/548 [00:31<00:46,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 219/548 [00:31<00:46,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 221/548 [00:31<00:45,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 223/548 [00:32<00:46,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 225/548 [00:32<00:46,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 227/548 [00:32<00:45,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 229/548 [00:32<00:45,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 231/548 [00:33<00:44,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 233/548 [00:33<00:44,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 235/548 [00:33<00:44,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 237/548 [00:34<00:43,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 239/548 [00:34<00:43,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 241/548 [00:34<00:42,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 243/548 [00:34<00:42,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 245/548 [00:35<00:42,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 247/548 [00:35<00:42,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 249/548 [00:35<00:41,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 251/548 [00:36<00:41,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 253/548 [00:36<00:41,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 255/548 [00:36<00:41,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 257/548 [00:36<00:40,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 259/548 [00:37<00:41,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 261/548 [00:37<00:40,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 263/548 [00:37<00:40,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 265/548 [00:38<00:40,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 267/548 [00:38<00:39,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 269/548 [00:38<00:39,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 271/548 [00:38<00:38,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 273/548 [00:39<00:38,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 275/548 [00:39<00:38,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 277/548 [00:39<00:38,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 279/548 [00:40<00:37,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 281/548 [00:40<00:38,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 283/548 [00:40<00:37,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 285/548 [00:40<00:37,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 287/548 [00:41<00:36,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 289/548 [00:41<00:37,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 291/548 [00:41<00:36,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 293/548 [00:42<00:35,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 295/548 [00:42<00:36,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 297/548 [00:42<00:35,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 299/548 [00:42<00:34,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 301/548 [00:43<00:35,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 300, Training Loss: 3.1734\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 303/548 [00:43<00:34,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 305/548 [00:43<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 307/548 [00:43<00:33,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 309/548 [00:44<00:33,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 311/548 [00:44<00:33,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 313/548 [00:44<00:33,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 315/548 [00:45<00:33,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 317/548 [00:45<00:32,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 319/548 [00:45<00:32,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▊    | 321/548 [00:45<00:32,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 323/548 [00:46<00:32,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 325/548 [00:46<00:31,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 327/548 [00:46<00:31,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 329/548 [00:47<00:30,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 331/548 [00:47<00:30,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 333/548 [00:47<00:30,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 335/548 [00:47<00:29,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 337/548 [00:48<00:29,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 339/548 [00:48<00:29,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 341/548 [00:48<00:29,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 343/548 [00:49<00:29,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 345/548 [00:49<00:28,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 347/548 [00:49<00:28,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 349/548 [00:49<00:27,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 351/548 [00:50<00:27,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 353/548 [00:50<00:27,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 355/548 [00:50<00:26,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 357/548 [00:51<00:26,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 359/548 [00:51<00:26,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 361/548 [00:51<00:26,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 363/548 [00:51<00:26,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 365/548 [00:52<00:25,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 367/548 [00:52<00:25,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 369/548 [00:52<00:25,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 371/548 [00:53<00:24,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 373/548 [00:53<00:24,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 375/548 [00:53<00:24,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 377/548 [00:53<00:23,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 379/548 [00:54<00:23,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 381/548 [00:54<00:23,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 383/548 [00:54<00:23,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 385/548 [00:54<00:22,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 387/548 [00:55<00:22,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 389/548 [00:55<00:22,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 391/548 [00:55<00:22,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 393/548 [00:56<00:21,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 395/548 [00:56<00:21,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 397/548 [00:56<00:21,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 399/548 [00:56<00:20,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 401/548 [00:57<00:20,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 400, Training Loss: 3.1763\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 403/548 [00:57<00:20,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 405/548 [00:57<00:20,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 407/548 [00:58<00:19,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 409/548 [00:58<00:19,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 411/548 [00:58<00:19,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 413/548 [00:58<00:19,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 415/548 [00:59<00:18,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 417/548 [00:59<00:18,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▋  | 419/548 [00:59<00:18,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 421/548 [01:00<00:17,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 423/548 [01:00<00:17,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 425/548 [01:00<00:17,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 427/548 [01:00<00:16,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 429/548 [01:01<00:16,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 431/548 [01:01<00:16,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 433/548 [01:01<00:16,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 435/548 [01:02<00:15,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 437/548 [01:02<00:15,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 439/548 [01:02<00:15,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 441/548 [01:02<00:14,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 443/548 [01:03<00:14,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 445/548 [01:03<00:14,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 447/548 [01:03<00:14,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 449/548 [01:03<00:13,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 451/548 [01:04<00:13,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 453/548 [01:04<00:13,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 455/548 [01:04<00:13,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 457/548 [01:05<00:12,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 459/548 [01:05<00:12,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 461/548 [01:05<00:12,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 463/548 [01:05<00:12,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 465/548 [01:06<00:11,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 467/548 [01:06<00:11,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 469/548 [01:06<00:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 471/548 [01:07<00:10,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 473/548 [01:07<00:10,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 475/548 [01:07<00:10,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 477/548 [01:07<00:09,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 479/548 [01:08<00:09,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 481/548 [01:08<00:09,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 483/548 [01:08<00:09,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▊ | 485/548 [01:09<00:08,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 487/548 [01:09<00:08,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 489/548 [01:09<00:08,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 491/548 [01:09<00:08,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 493/548 [01:10<00:07,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 495/548 [01:10<00:07,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 497/548 [01:10<00:07,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 499/548 [01:11<00:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 501/548 [01:11<00:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Batch 500, Training Loss: 3.1787\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 503/548 [01:11<00:06,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 505/548 [01:11<00:05,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 507/548 [01:12<00:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 509/548 [01:12<00:05,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 511/548 [01:12<00:05,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▎| 513/548 [01:13<00:04,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 515/548 [01:13<00:04,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 517/548 [01:13<00:04,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 519/548 [01:13<00:04,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 521/548 [01:14<00:03,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 523/548 [01:14<00:03,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 525/548 [01:14<00:03,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 527/548 [01:14<00:02,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 529/548 [01:15<00:02,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 531/548 [01:15<00:02,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 533/548 [01:15<00:02,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 535/548 [01:16<00:01,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 537/548 [01:16<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 539/548 [01:16<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 541/548 [01:16<00:00,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 543/548 [01:17<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 545/548 [01:17<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 547/548 [01:17<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([21, 256])\n",
      "text_features: torch.Size([21, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training Loss: 3.1763\n",
      "Epoch 6 Training Accuracy: 0.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|          | 1/118 [00:00<01:51,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|▎         | 3/118 [00:01<00:36,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|▍         | 5/118 [00:01<00:23,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|▌         | 7/118 [00:01<00:18,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|▊         | 9/118 [00:02<00:16,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|▉         | 11/118 [00:02<00:15,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 13/118 [00:02<00:14,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|█▎        | 15/118 [00:02<00:14,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█▍        | 17/118 [00:03<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 19/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  18%|█▊        | 21/118 [00:03<00:13,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  19%|█▉        | 23/118 [00:03<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 25/118 [00:04<00:12,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  23%|██▎       | 27/118 [00:04<00:12,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  25%|██▍       | 29/118 [00:04<00:12,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 31/118 [00:05<00:12,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  28%|██▊       | 33/118 [00:05<00:11,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  30%|██▉       | 35/118 [00:05<00:11,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  31%|███▏      | 37/118 [00:05<00:11,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  33%|███▎      | 39/118 [00:06<00:10,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  35%|███▍      | 41/118 [00:06<00:10,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  36%|███▋      | 43/118 [00:06<00:10,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 45/118 [00:07<00:10,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  40%|███▉      | 47/118 [00:07<00:09,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 49/118 [00:07<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  43%|████▎     | 51/118 [00:07<00:09,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  45%|████▍     | 53/118 [00:08<00:08,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 55/118 [00:08<00:08,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  48%|████▊     | 57/118 [00:08<00:08,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 59/118 [00:08<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  52%|█████▏    | 61/118 [00:09<00:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n",
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([32, 256])\n",
      "text_features: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Optional scheduler, can be added as needed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_and_validate(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     19\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[1;32m     20\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     21\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     22\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     24\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler\n\u001b[1;32m     25\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 308\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, device, loss_fn, optimizer, scheduler, print_every)\u001b[0m\n\u001b[1;32m    301\u001b[0m preds \u001b[38;5;241m=\u001b[39m model({\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m: pixel_values,\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: input_ids,\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask\n\u001b[1;32m    305\u001b[0m })\n\u001b[1;32m    307\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, labels)\n\u001b[0;32m--> 308\u001b[0m total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    310\u001b[0m val_preds\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39margmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    311\u001b[0m val_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the FLAVA model (make sure 'args' is already defined)\n",
    "model = FLAVAClassifier(args)\n",
    "\n",
    "# Define optimizer and optional scheduler\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if p.requires_grad]}\n",
    "]\n",
    "optimizer = AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = None  # Optional scheduler, can be added as needed\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=10,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b0c975c-9410-4123-81ba-8eed28abb0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/548 [00:01<04:17,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Training Loss: 3.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 102/548 [00:15<01:01,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Training Loss: 3.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 202/548 [00:29<00:48,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Training Loss: 3.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 302/548 [00:43<00:34,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Training Loss: 3.1952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 402/548 [00:57<00:20,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Training Loss: 3.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 502/548 [01:11<00:06,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Training Loss: 3.1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 3.1948\n",
      "Epoch 1 Training Accuracy: 0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 3.1793\n",
      "Epoch 1 Validation Accuracy: 0.1198\n",
      "Best model saved with validation accuracy: 0.1198\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/548 [00:00<03:56,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Training Loss: 3.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 102/548 [00:15<01:02,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Training Loss: 3.1868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 202/548 [00:29<00:49,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Training Loss: 3.1942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 302/548 [00:43<00:34,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Training Loss: 3.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 402/548 [00:57<00:20,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Training Loss: 3.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 502/548 [01:11<00:06,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Training Loss: 3.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 3.1848\n",
      "Epoch 2 Training Accuracy: 0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 3.1762\n",
      "Epoch 2 Validation Accuracy: 0.1198\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/548 [00:00<03:55,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Training Loss: 3.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 102/548 [00:15<01:04,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Training Loss: 3.1833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Optional scheduler, can be added as needed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_and_validate(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     19\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[1;32m     20\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     21\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     22\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     24\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler\n\u001b[1;32m     25\u001b[0m )\n",
      "Cell \u001b[0;32mIn[21], line 257\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, device, loss_fn, optimizer, scheduler, print_every)\u001b[0m\n\u001b[1;32m    254\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m preds \u001b[38;5;241m=\u001b[39m model({\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m: pixel_values,\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: input_ids,\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask\n\u001b[1;32m    261\u001b[0m })\n\u001b[1;32m    263\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, labels)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 181\u001b[0m, in \u001b[0;36mFLAVAClassifier.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_map(image_features)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#         print(f'image_features: {image_features.shape}')\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m         text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    182\u001b[0m         text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_map(text_features)\n\u001b[1;32m    184\u001b[0m         image_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(image_features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [batch_size, d]\u001b[39;00m\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:1052\u001b[0m, in \u001b[0;36mFlavaTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(\n\u001b[1;32m   1043\u001b[0m     attention_mask, input_shape, input_ids\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1047\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1048\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1049\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1050\u001b[0m )\n\u001b[0;32m-> 1052\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1053\u001b[0m     embedding_output,\n\u001b[1;32m   1054\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1055\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1056\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1057\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1058\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1059\u001b[0m )\n\u001b[1;32m   1060\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1061\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:668\u001b[0m, in \u001b[0;36mFlavaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    660\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    661\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    662\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m         output_attentions,\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n\u001b[1;32m    670\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/libraries/Canada_VoC_GPU_311_Dev/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:624\u001b[0m, in \u001b[0;36mFlavaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m    623\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m--> 624\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(layer_output)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[1;32m    627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(layer_output, hidden_states)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the FLAVA model (make sure 'args' is already defined)\n",
    "model = FLAVAClassifier(args)\n",
    "\n",
    "# Define optimizer and optional scheduler\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if p.requires_grad]}\n",
    "]\n",
    "optimizer = AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = None  # Optional scheduler, can be added as needed\n",
    "\n",
    "# Train the model\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=10,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d41bdf5-4cfa-4062-b535-415ba4d654fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900544"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7424*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78425813-ad9c-4439-a946-fe42e723b7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Canada_VoC_GPU_311_Dev",
   "language": "python",
   "name": "68488"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
